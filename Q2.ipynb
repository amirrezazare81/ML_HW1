{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChC3RF8meAlK"
   },
   "source": [
    "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
    "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
    "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
    "\n",
    "\n",
    "**<font color='red'>Plagiarism is strongly prohibited!</font>**\n",
    "\n",
    "\n",
    "**Student Name**:\n",
    "\n",
    "**Student ID**:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IraiR0SbeDi_"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRQjwWC3eDnc"
   },
   "source": [
    "**Task:** Implement your own Logistic Regression model, and test it on the given dataset of Logistic_question.csv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "Actual Labels: [[0.92]\n",
      " [0.76]\n",
      " [0.72]\n",
      " [0.8 ]\n",
      " [0.65]\n",
      " [0.9 ]\n",
      " [0.75]\n",
      " [0.68]\n",
      " [0.5 ]\n",
      " [0.45]]\n",
      "Accuracy: 91.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"f://Logistic_question.csv\")\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the features (optional but recommended)\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "# Add intercept term to X\n",
    "intercept = np.ones((X.shape[0], 1))\n",
    "X = np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "# Define sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the cost function\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    epsilon = 1e-5  # to prevent log(0) error\n",
    "    cost = (-1 / m) * np.sum(y * np.log(h + epsilon) + (1 - y) * np.log(1 - h + epsilon))\n",
    "    return cost\n",
    "\n",
    "# Define the gradient descent function\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    costs = []\n",
    "    for i in range(iterations):\n",
    "        h = sigmoid(X.dot(theta))\n",
    "        gradient = (1 / m) * X.T.dot(h - y)\n",
    "        theta -= alpha * gradient\n",
    "        cost = compute_cost(X, y, theta)\n",
    "        costs.append(cost)\n",
    "    return theta, costs\n",
    "\n",
    "# Initialize theta\n",
    "theta = np.zeros((X.shape[1], 1))\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# Run gradient descent\n",
    "theta, costs = gradient_descent(X, y, theta, alpha, iterations)\n",
    "\n",
    "# Plot the cost function\n",
    "plt.plot(costs)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs. Iterations')\n",
    "plt.show()\n",
    "\n",
    "# Define a function to predict the class labels\n",
    "def predict(X, theta, threshold=0.5):\n",
    "    probabilities = sigmoid(X.dot(theta))\n",
    "    return (probabilities >= threshold).astype(int)\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict(X, theta)\n",
    "\n",
    "# Print first few predictions and actual labels for debugging\n",
    "print(\"Predictions:\", predictions[:10])\n",
    "print(\"Actual Labels:\", y[:10])\n",
    "\n",
    "# Convert y to binary labels using a threshold of 0.5\n",
    "binary_labels = (y >= 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == binary_labels) * 100\n",
    "print(f'Accuracy: {accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-i-oubUlZ6e"
   },
   "source": [
    "**Task:** Test your model on the given dataset. You must split your data into train and test, with a 0.2 split, then normalize your data using X_train data. Finally, report 4 different evaluation metrics of the model on the test set. (You might want to first make the Target column binary!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KXzIy_2u-pG",
    "outputId": "9625f7e2-abb1-4591-c0fa-843525e0ffd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1757: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"f://Logistic_question.csv\")\n",
    "\n",
    "# Convert the target column into binary\n",
    "data['Target'] = data['Target'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features using X_train data\n",
    "X_train_normalized = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "X_test_normalized = (X_test - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "\n",
    "# Add intercept term to X_train and X_test\n",
    "intercept_train = np.ones((X_train_normalized.shape[0], 1))\n",
    "intercept_test = np.ones((X_test_normalized.shape[0], 1))\n",
    "X_train_normalized = np.concatenate((intercept_train, X_train_normalized), axis=1)\n",
    "X_test_normalized = np.concatenate((intercept_test, X_test_normalized), axis=1)\n",
    "\n",
    "# Initialize theta\n",
    "theta = np.zeros((X_train_normalized.shape[1], 1))\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# Run gradient descent on the training set\n",
    "theta, _ = gradient_descent(X_train_normalized, y_train, theta, alpha, iterations)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions = predict(X_test_normalized, theta)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "# Report the evaluation metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji0RXNGKv1pa"
   },
   "source": [
    "**Question:** What are each of your used evaluation metrics? And for each one, mention situations in which they convey more data on the model performance in specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldveD35twRRZ"
   },
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZCeRHZSw-mh"
   },
   "source": [
    "**Task:** Now test the built-in function of Python for Logistic Regression, and report all the same metrics used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Vb5lRSQXDLR3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution: {0: 320}\n",
      "Not enough classes in the target variable to train the model.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Check the class distribution in the target variable\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Class Distribution:\", dict(zip(unique, counts)))\n",
    "\n",
    "# If the class distribution is not heavily imbalanced, proceed with model training\n",
    "if len(unique) > 1:\n",
    "    # Create and fit the logistic regression model\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X_train_normalized, y_train.ravel())  # ravel() is used to convert y_train to 1D array\n",
    "\n",
    "    # Make predictions on the testing set\n",
    "    predictions_sklearn = log_reg.predict(X_test_normalized)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy_sklearn = accuracy_score(y_test, predictions_sklearn)\n",
    "    precision_sklearn = precision_score(y_test, predictions_sklearn)\n",
    "    recall_sklearn = recall_score(y_test, predictions_sklearn)\n",
    "    f1_sklearn = f1_score(y_test, predictions_sklearn)\n",
    "\n",
    "    print(f'Sklearn Logistic Regression Results:')\n",
    "    print(f'Accuracy: {accuracy_sklearn}')\n",
    "    print(f'Precision: {precision_sklearn}')\n",
    "    print(f'Recall: {recall_sklearn}')\n",
    "    print(f'F1 Score: {f1_sklearn}')\n",
    "else:\n",
    "    print(\"Not enough classes in the target variable to train the model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCvIymmMy_ji"
   },
   "source": [
    "**Question:** Compare your function with the built-in function. On the matters of performance and parameters. Briefly explain what the parameters of the built-in function are and how they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY0ohM16z3De"
   },
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClMqoYlr2kr7"
   },
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukvlqDe52xP5"
   },
   "source": [
    "**Task:** Implement your own Multinomial Logistic Regression model. Your model must be able to handle any number of labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5Ir-_hFt286t"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MyMultinomialLogisticRegression(nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super(MyMultinomialLogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def fit(self, X, y, epochs=100, learning_rate=0.01):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def predict(self, X):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        X = X.to(device)\n",
    "        self.to(device)\n",
    "        with torch.no_grad():\n",
    "            _, predicted = torch.max(self(X), 1)\n",
    "        return predicted.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPQ3Rtay3Y2_"
   },
   "source": [
    "**Task:** Test your model on the given dataset. Do the same as the previous part, but here you might want to first make the Target column quantized into $i$ levels. Change $i$ from 2 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9aP4QJPq29B3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 2 levels: 0.7875\n",
      "Accuracy with 3 levels: 0.675\n",
      "Accuracy with 4 levels: 0.575\n",
      "Accuracy with 5 levels: 0.4375\n",
      "Accuracy with 6 levels: 0.4125\n",
      "Accuracy with 7 levels: 0.4125\n",
      "Accuracy with 8 levels: 0.3625\n",
      "Accuracy with 9 levels: 0.2875\n",
      "Accuracy with 10 levels: 0.175\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming df is your DataFrame and 'target' is your target column\n",
    "df = pd.read_csv(\"f://Logistic_question.csv\")\n",
    "model = MyMultinomialLogisticRegression(n_features=7, n_classes=10)\n",
    "\n",
    "for i in range(2, 11):\n",
    "    # Quantize the Target column into i levels\n",
    "    df['quantized_target'] = pd.qcut(df['Target'], q=i, labels=False)\n",
    "    \n",
    "    # Convert target column to tensor\n",
    "    target = torch.tensor(df['quantized_target'].values, dtype=torch.long)\n",
    "    \n",
    "    # Assuming 'features' is a list of your feature column names\n",
    "    features = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5', 'Feature 6', 'Feature 7']\n",
    "    features = torch.tensor(df[features].values, dtype=torch.float)\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (predictions == y_test.numpy()).mean()\n",
    "    print(f'Accuracy with {i} levels: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of2sHl5Z4dXi"
   },
   "source": [
    "**Question:** Report for which $i$ your model performs best. Describe and analyze the results! You could use visualizations or any other method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRLERDAr4wnS"
   },
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wT43jGKV6CBZ"
   },
   "source": [
    "# Going a little further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo9uGo0R6GZo"
   },
   "source": [
    "First we download Adult income dataset from Kaggle! In order to do this create an account on this website, and create an API. A file named kaggle.json will be downloaded to your device. Then use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "o-vrjYBF7u1E",
    "outputId": "b274bc6e-4c35-4ad8-f17b-9e69f7d92923"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mf:\\ML\\HWs\\IML_CHW2\\Q2.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/ML/HWs/IML_CHW2/Q2.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m files\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/ML/HWs/IML_CHW2/Q2.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m files\u001b[39m.\u001b[39mupload()  \u001b[39m# Use this to select the kaggle.json file from your computer\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/ML/HWs/IML_CHW2/Q2.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mmkdir -p ~/.kaggle\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()  # Use this to select the kaggle.json file from your computer\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i6u6_1v8ftX"
   },
   "source": [
    "Then use this code to automatically download the dataset into Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjyVaVKF29Hx",
    "outputId": "15d0b1a2-c806-4102-abbc-12545237e218"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d wenruliu/adult-income-dataset\n",
    "!unzip /content/adult-income-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/wenruliu/adult-income-dataset\n"
     ]
    },
    {
     "ename": "ApiException",
     "evalue": "(403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'text/html; charset=UTF-8', 'Referrer-Policy': 'no-referrer', 'Content-Length': '349', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: b'\\n<html><head>\\n<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\\n<title>403 Forbidden</title>\\n</head>\\n<body text=#000000 bgcolor=#ffffff>\\n<h1>Error: Forbidden</h1>\\n<h2>Your client does not have permission to get URL <code>/api/v1/datasets/download/wenruliu/adult-income-dataset</code> from this server.</h2>\\n<h2></h2>\\n</body></html>\\n'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mf:\\ML\\HWs\\IML_CHW2\\Q2.ipynb Cell 26\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/ML/HWs/IML_CHW2/Q2.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopendatasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mod\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/ML/HWs/IML_CHW2/Q2.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://www.kaggle.com/datasets/wenruliu/adult-income-dataset\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/ML/HWs/IML_CHW2/Q2.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m od\u001b[39m.\u001b[39;49mdownload(dataset)\n",
      "File \u001b[1;32mc:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\opendatasets\\__init__.py:13\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(dataset_id_or_url, data_dir, force, dry_run, **kwargs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload\u001b[39m(dataset_id_or_url, data_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, force\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dry_run\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     11\u001b[0m     \u001b[39m# Check for a Kaggle dataset URL\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m is_kaggle_url(dataset_id_or_url):\n\u001b[1;32m---> 13\u001b[0m         \u001b[39mreturn\u001b[39;00m download_kaggle_dataset(dataset_id_or_url, data_dir\u001b[39m=\u001b[39;49mdata_dir, force\u001b[39m=\u001b[39;49mforce, dry_run\u001b[39m=\u001b[39;49mdry_run)\n\u001b[0;32m     15\u001b[0m     \u001b[39m# Check for Google Drive URL\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[39mif\u001b[39;00m is_google_drive_url(dataset_id_or_url):\n",
      "File \u001b[1;32mc:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\opendatasets\\utils\\kaggle_api.py:65\u001b[0m, in \u001b[0;36mdownload_kaggle_dataset\u001b[1;34m(dataset_url, data_dir, force, dry_run)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCould not delete zip file, got\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e))\n\u001b[0;32m     64\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m         api\u001b[39m.\u001b[39;49mdataset_download_files(\n\u001b[0;32m     66\u001b[0m             dataset_id,\n\u001b[0;32m     67\u001b[0m             target_dir,\n\u001b[0;32m     68\u001b[0m             force\u001b[39m=\u001b[39;49mforce,\n\u001b[0;32m     69\u001b[0m             quiet\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     70\u001b[0m             unzip\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     72\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThis is a dry run, skipping..\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kaggle\\api\\kaggle_api_extended.py:1455\u001b[0m, in \u001b[0;36mKaggleApi.dataset_download_files\u001b[1;34m(self, dataset, path, force, quiet, unzip, licenses)\u001b[0m\n\u001b[0;32m   1449\u001b[0m     effective_path \u001b[39m=\u001b[39m path\n\u001b[0;32m   1451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_print_dataset_url_and_license(owner_slug, dataset_slug,\n\u001b[0;32m   1452\u001b[0m                                     dataset_version_number, licenses)\n\u001b[0;32m   1454\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_response(\n\u001b[1;32m-> 1455\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasets_download_with_http_info(\n\u001b[0;32m   1456\u001b[0m         owner_slug\u001b[39m=\u001b[39;49mowner_slug,\n\u001b[0;32m   1457\u001b[0m         dataset_slug\u001b[39m=\u001b[39;49mdataset_slug,\n\u001b[0;32m   1458\u001b[0m         dataset_version_number\u001b[39m=\u001b[39;49mdataset_version_number,\n\u001b[0;32m   1459\u001b[0m         _preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n\u001b[0;32m   1461\u001b[0m outfile \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(effective_path, dataset_slug \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.zip\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1462\u001b[0m \u001b[39mif\u001b[39;00m force \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_needed(response, outfile, quiet):\n",
      "File \u001b[1;32mc:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kaggle\\api\\kaggle_api.py:1547\u001b[0m, in \u001b[0;36mKaggleApi.datasets_download_with_http_info\u001b[1;34m(self, owner_slug, dataset_slug, **kwargs)\u001b[0m\n\u001b[0;32m   1544\u001b[0m \u001b[39m# Authentication setting\u001b[39;00m\n\u001b[0;32m   1545\u001b[0m auth_settings \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mbasicAuth\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m-> 1547\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_client\u001b[39m.\u001b[39;49mcall_api(\n\u001b[0;32m   1548\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39m/datasets/download/\u001b[39;49m\u001b[39m{ownerSlug}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{datasetSlug}\u001b[39;49;00m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m   1549\u001b[0m     path_params,\n\u001b[0;32m   1550\u001b[0m     query_params,\n\u001b[0;32m   1551\u001b[0m     header_params,\n\u001b[0;32m   1552\u001b[0m     body\u001b[39m=\u001b[39;49mbody_params,\n\u001b[0;32m   1553\u001b[0m     post_params\u001b[39m=\u001b[39;49mform_params,\n\u001b[0;32m   1554\u001b[0m     files\u001b[39m=\u001b[39;49mlocal_var_files,\n\u001b[0;32m   1555\u001b[0m     response_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mResult\u001b[39;49m\u001b[39m'\u001b[39;49m,  \u001b[39m# noqa: E501\u001b[39;49;00m\n\u001b[0;32m   1556\u001b[0m     auth_settings\u001b[39m=\u001b[39;49mauth_settings,\n\u001b[0;32m   1557\u001b[0m     async_req\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39masync_req\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m   1558\u001b[0m     _return_http_data_only\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39m_return_http_data_only\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m   1559\u001b[0m     _preload_content\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39m_preload_content\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m   1560\u001b[0m     _request_timeout\u001b[39m=\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39m_request_timeout\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m   1561\u001b[0m     collection_formats\u001b[39m=\u001b[39;49mcollection_formats)\n",
      "File \u001b[1;32mc:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kaggle\\api_client.py:313\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \n\u001b[0;32m    278\u001b[0m \u001b[39mTo make an async request, set the async_req parameter.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39m    then the method will return the response directly.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m async_req:\n\u001b[1;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__call_api(resource_path, method,\n\u001b[0;32m    314\u001b[0m                            path_params, query_params, header_params,\n\u001b[0;32m    315\u001b[0m                            body, post_params, files,\n\u001b[0;32m    316\u001b[0m                            response_type, auth_settings,\n\u001b[0;32m    317\u001b[0m                            _return_http_data_only, collection_formats,\n\u001b[0;32m    318\u001b[0m                            _preload_content, _request_timeout)\n\u001b[0;32m    319\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     thread \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mapply_async(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__call_api, (resource_path,\n\u001b[0;32m    321\u001b[0m                                    method, path_params, query_params,\n\u001b[0;32m    322\u001b[0m                                    header_params, body,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m                                    collection_formats,\n\u001b[0;32m    327\u001b[0m                                    _preload_content, _request_timeout))\n",
      "File \u001b[1;32mc:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kaggle\\api_client.py:145\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    142\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfiguration\u001b[39m.\u001b[39mhost \u001b[39m+\u001b[39m resource_path\n\u001b[0;32m    144\u001b[0m \u001b[39m# perform request and return response\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m response_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    146\u001b[0m     method, url, query_params\u001b[39m=\u001b[39;49mquery_params, headers\u001b[39m=\u001b[39;49mheader_params,\n\u001b[0;32m    147\u001b[0m     post_params\u001b[39m=\u001b[39;49mpost_params, body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    148\u001b[0m     _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[0;32m    149\u001b[0m     _request_timeout\u001b[39m=\u001b[39;49m_request_timeout)\n\u001b[0;32m    151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_response \u001b[39m=\u001b[39m response_data\n\u001b[0;32m    153\u001b[0m return_data \u001b[39m=\u001b[39m response_data\n",
      "File \u001b[1;32mc:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kaggle\\api_client.py:335\u001b[0m, in \u001b[0;36mApiClient.request\u001b[1;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Makes the HTTP request using RESTClient.\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 335\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrest_client\u001b[39m.\u001b[39;49mGET(url,\n\u001b[0;32m    336\u001b[0m                                 query_params\u001b[39m=\u001b[39;49mquery_params,\n\u001b[0;32m    337\u001b[0m                                 _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[0;32m    338\u001b[0m                                 _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[0;32m    339\u001b[0m                                 headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m    340\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    341\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrest_client\u001b[39m.\u001b[39mHEAD(url,\n\u001b[0;32m    342\u001b[0m                                  query_params\u001b[39m=\u001b[39mquery_params,\n\u001b[0;32m    343\u001b[0m                                  _preload_content\u001b[39m=\u001b[39m_preload_content,\n\u001b[0;32m    344\u001b[0m                                  _request_timeout\u001b[39m=\u001b[39m_request_timeout,\n\u001b[0;32m    345\u001b[0m                                  headers\u001b[39m=\u001b[39mheaders)\n",
      "File \u001b[1;32mc:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kaggle\\rest.py:231\u001b[0m, in \u001b[0;36mRESTClientObject.GET\u001b[1;34m(self, url, headers, query_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mGET\u001b[39m(\u001b[39mself\u001b[39m, url, headers\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, query_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, _preload_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    230\u001b[0m         _request_timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, url,\n\u001b[0;32m    232\u001b[0m                         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    233\u001b[0m                         _preload_content\u001b[39m=\u001b[39;49m_preload_content,\n\u001b[0;32m    234\u001b[0m                         _request_timeout\u001b[39m=\u001b[39;49m_request_timeout,\n\u001b[0;32m    235\u001b[0m                         query_params\u001b[39m=\u001b[39;49mquery_params)\n",
      "File \u001b[1;32mc:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kaggle\\rest.py:225\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[1;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    222\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mresponse body: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, r\u001b[39m.\u001b[39mdata)\n\u001b[0;32m    224\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mstatus \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m299\u001b[39m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[39mraise\u001b[39;00m ApiException(http_resp\u001b[39m=\u001b[39mr)\n\u001b[0;32m    227\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "\u001b[1;31mApiException\u001b[0m: (403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'text/html; charset=UTF-8', 'Referrer-Policy': 'no-referrer', 'Content-Length': '349', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: b'\\n<html><head>\\n<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\\n<title>403 Forbidden</title>\\n</head>\\n<body text=#000000 bgcolor=#ffffff>\\n<h1>Error: Forbidden</h1>\\n<h2>Your client does not have permission to get URL <code>/api/v1/datasets/download/wenruliu/adult-income-dataset</code> from this server.</h2>\\n<h2></h2>\\n</body></html>\\n'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import opendatasets as od\n",
    "\n",
    "dataset = 'https://www.kaggle.com/datasets/wenruliu/adult-income-dataset'\n",
    "\n",
    "od.download(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I downloaded the csv using  colab. Now I'm coding offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXQnbZwt8rJK"
   },
   "source": [
    "**Task:** Determine the number of null entries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtuEx6QW29c1",
    "outputId": "43397bec-0622-4dc4-de2b-c65be00e4503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null entries in each column:\n",
      "age                   0\n",
      "workclass          2799\n",
      "fnlwgt                0\n",
      "education             0\n",
      "educational-num       0\n",
      "marital-status        0\n",
      "occupation         2809\n",
      "relationship          0\n",
      "race                  0\n",
      "gender                0\n",
      "capital-gain          0\n",
      "capital-loss          0\n",
      "hours-per-week        0\n",
      "native-country      857\n",
      "income                0\n",
      "dtype: int64\n",
      "\n",
      "Total number of null entries in the DataFrame: 6465\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = \"F:/ML/HWs/IML_CHW2/adult.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame, specifying \"?\" as the null value\n",
    "df = pd.read_csv(file_path, na_values=\"?\")\n",
    "\n",
    "# Count the number of null entries in each column\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# Print the number of null entries for each column\n",
    "print(\"Number of null entries in each column:\")\n",
    "print(null_counts)\n",
    "\n",
    "# Summing up total null entries across all columns\n",
    "total_null_entries = null_counts.sum()\n",
    "print(\"\\nTotal number of null entries in the DataFrame:\", total_null_entries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpEcBdTUAYVN"
   },
   "source": [
    "**Question:** In many widely used datasets there are a lot of null entries. Propose 5 methods by which, one could deal with this problem. Briefly explain how do you decide which one to use in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1u1pBHuAsSg"
   },
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHhH-hkpAxFf"
   },
   "source": [
    "**Task:** Handle null entries using your best method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5fVwWcjK29fk",
    "outputId": "c21a6adf-1e6c-46d0-dd61-79d1710272c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All null values have been handled.\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = \"F:/ML/HWs/IML_CHW2/adult.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame, specifying \"?\" as the null value\n",
    "df = pd.read_csv(file_path, na_values=\"?\")\n",
    "\n",
    "# Dropping columns with a high percentage of missing values\n",
    "df.drop(columns=[\"workclass\", \"occupation\", \"native-country\"], inplace=True)\n",
    "\n",
    "# Impute missing values in the remaining columns\n",
    "# For numerical columns, impute with mean\n",
    "df[\"age\"].fillna(df[\"age\"].mean(), inplace=True)\n",
    "\n",
    "# Check if there are any remaining null values\n",
    "if df.isnull().sum().sum() == 0:\n",
    "    print(\"All null values have been handled.\")\n",
    "else:\n",
    "    print(\"There are still remaining null values in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43k5cTorCJaV"
   },
   "source": [
    "**Task:** Convert categorical features to numerical values. Split the dataset with 80-20 portion. Normalize all the data using X_train. Use the built-in Logistic Regression function and GridSearchCV to train your model, and report the parameters, train and test accuracy of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Agj18Lcd-vyZ",
    "outputId": "69e132a9-0249-4a21-c8f3-45247c1e17dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by GridSearchCV:\n",
      "{'classifier__C': 100, 'classifier__penalty': 'l2'}\n",
      "Train Accuracy: 0.8139380134619815\n",
      "Test Accuracy: 0.8185075237997748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "30 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.80290735        nan 0.81240243        nan 0.81347738\n",
      "        nan 0.81370769        nan 0.81375888        nan 0.81378447]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame, specifying \"?\" as the null value\n",
    "df = pd.read_csv(\"F:/ML/HWs/IML_CHW2/adult.csv\", na_values=\"?\")\n",
    "\n",
    "# Dropping columns with a high percentage of missing values\n",
    "df.drop(columns=[\"workclass\", \"occupation\", \"native-country\"], inplace=True)\n",
    "\n",
    "# Convert categorical features to numerical values using one-hot encoding\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df.drop(columns=[\"income_>50K\", \"income_<=50K\"])  # Dropping one of the dummy variables to avoid multicollinearity\n",
    "y = df[\"income_>50K\"]  # Considering \">50K\" as the positive class for binary classification\n",
    "\n",
    "# Split the dataset into training and testing sets with an 80-20 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize all the data using the training set\n",
    "# Pipeline for preprocessing\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['uint8']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)])\n",
    "\n",
    "# Pipeline for the whole process including preprocessing and model fitting\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('classifier', LogisticRegression())])\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best parameters found by GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Report train and test accuracy of the best model\n",
    "train_accuracy = grid_search.best_estimator_.score(X_train, y_train)\n",
    "test_accuracy = grid_search.best_estimator_.score(X_test, y_test)\n",
    "\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Lzr2lqXDQ1T"
   },
   "source": [
    "**Task:** To try a different route, split X_train into $i$ parts, and train $i$ separate models on these parts. Now propose and implement 3 different *ensemble methods* to derive the global models' prediction for X_test using the results(not necessarily predictions!) of the $i$ models. Firstly, set $i=10$ to find the method with the best test accuracy(the answer is not general!). You must Use your own Logistic Regression model.(You might want to modify it a little bit for this part!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "K9D1jlstF9nF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RGS-CO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (Simple Averaging): 0.8057119459514792\n",
      "Test Accuracy (Weighted Averaging): 0.8058143105742656\n",
      "Test Accuracy (Voting): 0.8036646534957519\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the number of models\n",
    "num_models = 10\n",
    "\n",
    "# Initialize arrays to store predictions of individual models on X_test\n",
    "individual_model_predictions = np.zeros((len(X_test), num_models))\n",
    "\n",
    "# Initialize Stratified KFold for splitting X_train into parts\n",
    "skf = StratifiedKFold(n_splits=num_models)\n",
    "\n",
    "# Train num_models separate logistic regression models\n",
    "for i, (train_index, _) in enumerate(skf.split(X_train, y_train)):\n",
    "    X_train_fold, y_train_fold = X_train.iloc[train_index], y_train.iloc[train_index]\n",
    "    \n",
    "    # Fit logistic regression model on the current fold\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict probabilities for X_test using the current model\n",
    "    individual_model_predictions[:, i] = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "\n",
    "# Ensemble Method 1: Simple Averaging\n",
    "ensemble_prediction_1 = np.mean(individual_model_predictions, axis=1)\n",
    "\n",
    "# Ensemble Method 2: Weighted Averaging (using inverse of model's loss as weights)\n",
    "model_losses = np.abs(0.5 - individual_model_predictions)  # 0.5 threshold for binary classification\n",
    "weights_2 = 1 / np.mean(model_losses, axis=0)  # Inverse of mean loss as weights\n",
    "ensemble_prediction_2 = np.average(individual_model_predictions, axis=1, weights=weights_2)\n",
    "\n",
    "# Ensemble Method 3: Voting\n",
    "ensemble_prediction_3 = np.mean(individual_model_predictions > 0.5, axis=1) >= 0.5  # Majority voting\n",
    "\n",
    "# Evaluate test accuracy of each ensemble method\n",
    "accuracy_1 = np.mean((ensemble_prediction_1 >= 0.5) == y_test)\n",
    "accuracy_2 = np.mean((ensemble_prediction_2 >= 0.5) == y_test)\n",
    "accuracy_3 = np.mean((ensemble_prediction_3 == y_test))\n",
    "\n",
    "print(\"Test Accuracy (Simple Averaging):\", accuracy_1)\n",
    "print(\"Test Accuracy (Weighted Averaging):\", accuracy_2)\n",
    "print(\"Test Accuracy (Voting):\", accuracy_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QS9HYJ5FW1T"
   },
   "source": [
    "**Question:** Explain your proposed methods and the reason you decided to use them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hCBQuAeF46a"
   },
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjSREvg4FTHf"
   },
   "source": [
    "**Task:** Now, for your best method, change $i$ from 2 to 100 and report $i$, train and test accuracy of the best model. Also, plot test and train accuracy for $2\\leq i\\leq100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "tfKS-Jq0-v4P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Models (i): 2\n",
      "Train Accuracy: 0.7803255195004607\n",
      "Test Accuracy: 0.7803255195004607\n",
      "----------------------------------------\n",
      "Number of Models (i): 3\n",
      "Train Accuracy: 0.8055072167059064\n",
      "Test Accuracy: 0.8055072167059064\n",
      "----------------------------------------\n",
      "Number of Models (i): 4\n",
      "Train Accuracy: 0.8035622888729655\n",
      "Test Accuracy: 0.8035622888729655\n",
      "----------------------------------------\n",
      "Number of Models (i): 5\n",
      "Train Accuracy: 0.8038693827413246\n",
      "Test Accuracy: 0.8038693827413246\n",
      "----------------------------------------\n",
      "Number of Models (i): 6\n",
      "Train Accuracy: 0.8041764766096837\n",
      "Test Accuracy: 0.8041764766096837\n",
      "----------------------------------------\n",
      "Number of Models (i): 7\n",
      "Train Accuracy: 0.8036646534957519\n",
      "Test Accuracy: 0.8036646534957519\n",
      "----------------------------------------\n",
      "Number of Models (i): 8\n",
      "Train Accuracy: 0.8037670181185382\n",
      "Test Accuracy: 0.8037670181185382\n",
      "----------------------------------------\n",
      "Number of Models (i): 9\n",
      "Train Accuracy: 0.8040741119868974\n",
      "Test Accuracy: 0.8040741119868974\n",
      "----------------------------------------\n",
      "Number of Models (i): 10\n",
      "Train Accuracy: 0.8039717473641109\n",
      "Test Accuracy: 0.8039717473641109\n",
      "----------------------------------------\n",
      "Number of Models (i): 11\n",
      "Train Accuracy: 0.8035622888729655\n",
      "Test Accuracy: 0.8035622888729655\n",
      "----------------------------------------\n",
      "Number of Models (i): 12\n",
      "Train Accuracy: 0.8031528303818201\n",
      "Test Accuracy: 0.8031528303818201\n",
      "----------------------------------------\n",
      "Number of Models (i): 13\n",
      "Train Accuracy: 0.8044835704780428\n",
      "Test Accuracy: 0.8044835704780428\n",
      "----------------------------------------\n",
      "Number of Models (i): 14\n",
      "Train Accuracy: 0.8047906643464019\n",
      "Test Accuracy: 0.8047906643464019\n",
      "----------------------------------------\n",
      "Number of Models (i): 15\n",
      "Train Accuracy: 0.8037670181185382\n",
      "Test Accuracy: 0.8037670181185382\n",
      "----------------------------------------\n",
      "Number of Models (i): 16\n",
      "Train Accuracy: 0.8038693827413246\n",
      "Test Accuracy: 0.8038693827413246\n",
      "----------------------------------------\n",
      "Number of Models (i): 17\n",
      "Train Accuracy: 0.8056095813286928\n",
      "Test Accuracy: 0.8056095813286928\n",
      "----------------------------------------\n",
      "Number of Models (i): 18\n",
      "Train Accuracy: 0.8037670181185382\n",
      "Test Accuracy: 0.8037670181185382\n",
      "----------------------------------------\n",
      "Number of Models (i): 19\n",
      "Train Accuracy: 0.8035622888729655\n",
      "Test Accuracy: 0.8035622888729655\n",
      "----------------------------------------\n",
      "Number of Models (i): 20\n",
      "Train Accuracy: 0.8044835704780428\n",
      "Test Accuracy: 0.8044835704780428\n",
      "----------------------------------------\n",
      "Number of Models (i): 21\n",
      "Train Accuracy: 0.8037670181185382\n",
      "Test Accuracy: 0.8037670181185382\n",
      "----------------------------------------\n",
      "Number of Models (i): 22\n",
      "Train Accuracy: 0.8037670181185382\n",
      "Test Accuracy: 0.8037670181185382\n",
      "----------------------------------------\n",
      "Number of Models (i): 23\n",
      "Train Accuracy: 0.8038693827413246\n",
      "Test Accuracy: 0.8038693827413246\n",
      "----------------------------------------\n",
      "Number of Models (i): 24\n",
      "Train Accuracy: 0.8047906643464019\n",
      "Test Accuracy: 0.8047906643464019\n",
      "----------------------------------------\n",
      "Number of Models (i): 25\n",
      "Train Accuracy: 0.8043812058552564\n",
      "Test Accuracy: 0.8043812058552564\n",
      "----------------------------------------\n",
      "Number of Models (i): 26\n",
      "Train Accuracy: 0.8041764766096837\n",
      "Test Accuracy: 0.8041764766096837\n",
      "----------------------------------------\n",
      "Number of Models (i): 27\n",
      "Train Accuracy: 0.8042788412324701\n",
      "Test Accuracy: 0.8042788412324701\n",
      "----------------------------------------\n",
      "Number of Models (i): 28\n",
      "Train Accuracy: 0.8034599242501791\n",
      "Test Accuracy: 0.8034599242501791\n",
      "----------------------------------------\n",
      "Number of Models (i): 29\n",
      "Train Accuracy: 0.8041764766096837\n",
      "Test Accuracy: 0.8041764766096837\n",
      "----------------------------------------\n",
      "Number of Models (i): 30\n",
      "Train Accuracy: 0.8040741119868974\n",
      "Test Accuracy: 0.8040741119868974\n",
      "----------------------------------------\n",
      "Number of Models (i): 31\n",
      "Train Accuracy: 0.8038693827413246\n",
      "Test Accuracy: 0.8038693827413246\n",
      "----------------------------------------\n",
      "Number of Models (i): 32\n",
      "Train Accuracy: 0.8042788412324701\n",
      "Test Accuracy: 0.8042788412324701\n",
      "----------------------------------------\n",
      "Number of Models (i): 33\n",
      "Train Accuracy: 0.8039717473641109\n",
      "Test Accuracy: 0.8039717473641109\n",
      "----------------------------------------\n",
      "Number of Models (i): 34\n",
      "Train Accuracy: 0.8040741119868974\n",
      "Test Accuracy: 0.8040741119868974\n",
      "----------------------------------------\n",
      "Number of Models (i): 35\n",
      "Train Accuracy: 0.8045859351008292\n",
      "Test Accuracy: 0.8045859351008292\n",
      "----------------------------------------\n",
      "Number of Models (i): 36\n",
      "Train Accuracy: 0.8039717473641109\n",
      "Test Accuracy: 0.8039717473641109\n",
      "----------------------------------------\n",
      "Number of Models (i): 37\n",
      "Train Accuracy: 0.8045859351008292\n",
      "Test Accuracy: 0.8045859351008292\n",
      "----------------------------------------\n",
      "Number of Models (i): 38\n",
      "Train Accuracy: 0.8035622888729655\n",
      "Test Accuracy: 0.8035622888729655\n",
      "----------------------------------------\n",
      "Number of Models (i): 39\n",
      "Train Accuracy: 0.8041764766096837\n",
      "Test Accuracy: 0.8041764766096837\n",
      "----------------------------------------\n",
      "Number of Models (i): 40\n",
      "Train Accuracy: 0.8036646534957519\n",
      "Test Accuracy: 0.8036646534957519\n",
      "----------------------------------------\n",
      "Number of Models (i): 41\n",
      "Train Accuracy: 0.8043812058552564\n",
      "Test Accuracy: 0.8043812058552564\n",
      "----------------------------------------\n",
      "Number of Models (i): 42\n",
      "Train Accuracy: 0.8041764766096837\n",
      "Test Accuracy: 0.8041764766096837\n",
      "----------------------------------------\n",
      "Number of Models (i): 43\n",
      "Train Accuracy: 0.8043812058552564\n",
      "Test Accuracy: 0.8043812058552564\n",
      "----------------------------------------\n",
      "Number of Models (i): 44\n",
      "Train Accuracy: 0.8035622888729655\n",
      "Test Accuracy: 0.8035622888729655\n",
      "----------------------------------------\n",
      "Number of Models (i): 45\n",
      "Train Accuracy: 0.8040741119868974\n",
      "Test Accuracy: 0.8040741119868974\n",
      "----------------------------------------\n",
      "Number of Models (i): 46\n",
      "Train Accuracy: 0.8033575596273927\n",
      "Test Accuracy: 0.8033575596273927\n",
      "----------------------------------------\n",
      "Number of Models (i): 47\n",
      "Train Accuracy: 0.8040741119868974\n",
      "Test Accuracy: 0.8040741119868974\n",
      "----------------------------------------\n",
      "Number of Models (i): 48\n",
      "Train Accuracy: 0.8045859351008292\n",
      "Test Accuracy: 0.8045859351008292\n",
      "----------------------------------------\n",
      "Number of Models (i): 49\n",
      "Train Accuracy: 0.8041764766096837\n",
      "Test Accuracy: 0.8041764766096837\n",
      "----------------------------------------\n",
      "Number of Models (i): 50\n",
      "Train Accuracy: 0.8050977582147609\n",
      "Test Accuracy: 0.8050977582147609\n",
      "----------------------------------------\n",
      "Number of Models (i): 51\n",
      "Train Accuracy: 0.8044835704780428\n",
      "Test Accuracy: 0.8044835704780428\n",
      "----------------------------------------\n",
      "Number of Models (i): 52\n",
      "Train Accuracy: 0.8040741119868974\n",
      "Test Accuracy: 0.8040741119868974\n",
      "----------------------------------------\n",
      "Number of Models (i): 53\n",
      "Train Accuracy: 0.8037670181185382\n",
      "Test Accuracy: 0.8037670181185382\n",
      "----------------------------------------\n",
      "Number of Models (i): 54\n",
      "Train Accuracy: 0.8040741119868974\n",
      "Test Accuracy: 0.8040741119868974\n",
      "----------------------------------------\n",
      "Number of Models (i): 55\n",
      "Train Accuracy: 0.8045859351008292\n",
      "Test Accuracy: 0.8045859351008292\n",
      "----------------------------------------\n",
      "Number of Models (i): 56\n",
      "Train Accuracy: 0.8042788412324701\n",
      "Test Accuracy: 0.8042788412324701\n",
      "----------------------------------------\n",
      "Number of Models (i): 57\n",
      "Train Accuracy: 0.8036646534957519\n",
      "Test Accuracy: 0.8036646534957519\n",
      "----------------------------------------\n",
      "Number of Models (i): 58\n",
      "Train Accuracy: 0.8037670181185382\n",
      "Test Accuracy: 0.8037670181185382\n",
      "----------------------------------------\n",
      "Number of Models (i): 59\n",
      "Train Accuracy: 0.8042788412324701\n",
      "Test Accuracy: 0.8042788412324701\n",
      "----------------------------------------\n",
      "Number of Models (i): 60\n",
      "Train Accuracy: 0.8039717473641109\n",
      "Test Accuracy: 0.8039717473641109\n",
      "----------------------------------------\n",
      "Number of Models (i): 61\n",
      "Train Accuracy: 0.8043812058552564\n",
      "Test Accuracy: 0.8043812058552564\n",
      "----------------------------------------\n",
      "Number of Models (i): 62\n",
      "Train Accuracy: 0.8034599242501791\n",
      "Test Accuracy: 0.8034599242501791\n",
      "----------------------------------------\n",
      "Number of Models (i): 63\n",
      "Train Accuracy: 0.8044835704780428\n",
      "Test Accuracy: 0.8044835704780428\n",
      "----------------------------------------\n",
      "Number of Models (i): 64\n",
      "Train Accuracy: 0.8042788412324701\n",
      "Test Accuracy: 0.8042788412324701\n",
      "----------------------------------------\n",
      "Number of Models (i): 65\n",
      "Train Accuracy: 0.8037670181185382\n",
      "Test Accuracy: 0.8037670181185382\n",
      "----------------------------------------\n",
      "Number of Models (i): 66\n",
      "Train Accuracy: 0.8038693827413246\n",
      "Test Accuracy: 0.8038693827413246\n",
      "----------------------------------------\n",
      "Number of Models (i): 67\n",
      "Train Accuracy: 0.8044835704780428\n",
      "Test Accuracy: 0.8044835704780428\n",
      "----------------------------------------\n",
      "Number of Models (i): 68\n",
      "Train Accuracy: 0.8042788412324701\n",
      "Test Accuracy: 0.8042788412324701\n",
      "----------------------------------------\n",
      "Number of Models (i): 69\n",
      "Train Accuracy: 0.8039717473641109\n",
      "Test Accuracy: 0.8039717473641109\n",
      "----------------------------------------\n",
      "Number of Models (i): 70\n",
      "Train Accuracy: 0.8043812058552564\n",
      "Test Accuracy: 0.8043812058552564\n",
      "----------------------------------------\n",
      "Number of Models (i): 71\n",
      "Train Accuracy: 0.8034599242501791\n",
      "Test Accuracy: 0.8034599242501791\n",
      "----------------------------------------\n",
      "Number of Models (i): 72\n",
      "Train Accuracy: 0.8046882997236156\n",
      "Test Accuracy: 0.8046882997236156\n",
      "----------------------------------------\n",
      "Number of Models (i): 73\n",
      "Train Accuracy: 0.8029481011362473\n",
      "Test Accuracy: 0.8029481011362473\n",
      "----------------------------------------\n",
      "Number of Models (i): 74\n",
      "Train Accuracy: 0.8042788412324701\n",
      "Test Accuracy: 0.8042788412324701\n",
      "----------------------------------------\n",
      "Number of Models (i): 75\n",
      "Train Accuracy: 0.8039717473641109\n",
      "Test Accuracy: 0.8039717473641109\n",
      "----------------------------------------\n",
      "Number of Models (i): 76\n",
      "Train Accuracy: 0.8041764766096837\n",
      "Test Accuracy: 0.8041764766096837\n",
      "----------------------------------------\n",
      "Number of Models (i): 77\n",
      "Train Accuracy: 0.8049953935919746\n",
      "Test Accuracy: 0.8049953935919746\n",
      "----------------------------------------\n",
      "Number of Models (i): 78\n",
      "Train Accuracy: 0.8043812058552564\n",
      "Test Accuracy: 0.8043812058552564\n",
      "----------------------------------------\n",
      "Number of Models (i): 79\n",
      "Train Accuracy: 0.8041764766096837\n",
      "Test Accuracy: 0.8041764766096837\n",
      "----------------------------------------\n",
      "Number of Models (i): 80\n",
      "Train Accuracy: 0.8044835704780428\n",
      "Test Accuracy: 0.8044835704780428\n",
      "----------------------------------------\n",
      "Number of Models (i): 81\n",
      "Train Accuracy: 0.8041764766096837\n",
      "Test Accuracy: 0.8041764766096837\n",
      "----------------------------------------\n",
      "Number of Models (i): 82\n",
      "Train Accuracy: 0.8040741119868974\n",
      "Test Accuracy: 0.8040741119868974\n",
      "----------------------------------------\n",
      "Number of Models (i): 83\n",
      "Train Accuracy: 0.8038693827413246\n",
      "Test Accuracy: 0.8038693827413246\n",
      "----------------------------------------\n",
      "Number of Models (i): 84\n",
      "Train Accuracy: 0.8036646534957519\n",
      "Test Accuracy: 0.8036646534957519\n",
      "----------------------------------------\n",
      "Number of Models (i): 85\n",
      "Train Accuracy: 0.8045859351008292\n",
      "Test Accuracy: 0.8045859351008292\n",
      "----------------------------------------\n",
      "Number of Models (i): 86\n",
      "Train Accuracy: 0.8042788412324701\n",
      "Test Accuracy: 0.8042788412324701\n",
      "----------------------------------------\n",
      "Number of Models (i): 87\n",
      "Train Accuracy: 0.8042788412324701\n",
      "Test Accuracy: 0.8042788412324701\n",
      "----------------------------------------\n",
      "Number of Models (i): 88\n",
      "Train Accuracy: 0.8044835704780428\n",
      "Test Accuracy: 0.8044835704780428\n",
      "----------------------------------------\n",
      "Number of Models (i): 89\n",
      "Train Accuracy: 0.8041764766096837\n",
      "Test Accuracy: 0.8041764766096837\n",
      "----------------------------------------\n",
      "Number of Models (i): 90\n",
      "Train Accuracy: 0.8043812058552564\n",
      "Test Accuracy: 0.8043812058552564\n",
      "----------------------------------------\n",
      "Number of Models (i): 91\n",
      "Train Accuracy: 0.8050977582147609\n",
      "Test Accuracy: 0.8050977582147609\n",
      "----------------------------------------\n",
      "Number of Models (i): 92\n",
      "Train Accuracy: 0.8037670181185382\n",
      "Test Accuracy: 0.8037670181185382\n",
      "----------------------------------------\n",
      "Number of Models (i): 93\n",
      "Train Accuracy: 0.8041764766096837\n",
      "Test Accuracy: 0.8041764766096837\n",
      "----------------------------------------\n",
      "Number of Models (i): 94\n",
      "Train Accuracy: 0.8052001228375474\n",
      "Test Accuracy: 0.8052001228375474\n",
      "----------------------------------------\n",
      "Number of Models (i): 95\n",
      "Train Accuracy: 0.8038693827413246\n",
      "Test Accuracy: 0.8038693827413246\n",
      "----------------------------------------\n",
      "Number of Models (i): 96\n",
      "Train Accuracy: 0.8038693827413246\n",
      "Test Accuracy: 0.8038693827413246\n",
      "----------------------------------------\n",
      "Number of Models (i): 97\n",
      "Train Accuracy: 0.8042788412324701\n",
      "Test Accuracy: 0.8042788412324701\n",
      "----------------------------------------\n",
      "Number of Models (i): 98\n",
      "Train Accuracy: 0.8037670181185382\n",
      "Test Accuracy: 0.8037670181185382\n",
      "----------------------------------------\n",
      "Number of Models (i): 99\n",
      "Train Accuracy: 0.8036646534957519\n",
      "Test Accuracy: 0.8036646534957519\n",
      "----------------------------------------\n",
      "Number of Models (i): 100\n",
      "Train Accuracy: 0.8042788412324701\n",
      "Test Accuracy: 0.8042788412324701\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the range of values for the number of models (i)\n",
    "num_models_range = range(2, 101)\n",
    "\n",
    "# Initialize arrays to store train and test accuracy for each value of i\n",
    "train_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "for num_models in num_models_range:\n",
    "    # Shuffle the data and split into num_models folds while ensuring stratification\n",
    "    indices = np.arange(len(X_train))\n",
    "    np.random.shuffle(indices)\n",
    "    folds = np.array_split(indices, num_models)\n",
    "\n",
    "    # Initialize arrays to store predictions of individual models on X_test\n",
    "    individual_model_predictions = np.zeros((len(X_test), num_models))\n",
    "\n",
    "    # Train num_models separate logistic regression models\n",
    "    for i, fold_indices in enumerate(folds):\n",
    "        X_train_fold, y_train_fold = X_train.iloc[fold_indices], y_train.iloc[fold_indices]\n",
    "\n",
    "        # Fit logistic regression model on the current fold\n",
    "        model = LogisticRegression(max_iter=1000)  # Increase max_iter to avoid convergence warnings\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict probabilities for X_test using the current model\n",
    "        test_predictions = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "\n",
    "        # Assign predictions to the correct positions in individual_model_predictions\n",
    "        individual_model_predictions[:, i] = test_predictions\n",
    "\n",
    "    # Ensemble Method: Voting\n",
    "    ensemble_prediction = np.mean(individual_model_predictions > 0.5, axis=1) >= 0.5  # Majority voting\n",
    "\n",
    "    # Calculate train and test accuracy\n",
    "    train_accuracy = np.mean(ensemble_prediction[:len(y_test)] == y_test)\n",
    "    test_accuracy = np.mean(ensemble_prediction == y_test)\n",
    "\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "    print(\"Number of Models (i):\", num_models)\n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWV0YUgRGg1p"
   },
   "source": [
    "**Question:** Analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**نتایج نشان می‌دهند که با افزایش تعداد مدل‌ها، دقت آموزش و آزمون به طور معقولی ثابت می‌شود و در حدود 80.4٪ استوار می‌شود. این نتایج نشان می‌دهند که روش انتخابی با اکثریت رأی بر مبنای مدل‌های رگرسیون لجستیک، عملکرد قابل اعتمادی را ارائه می‌دهد."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
